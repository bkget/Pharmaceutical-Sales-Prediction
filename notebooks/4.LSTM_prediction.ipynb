{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "AibSpKjWvhLB"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from statsmodels.tsa.stattools import adfuller \n",
        "from sklearn.preprocessing import MinMaxScaler \n",
        "import tensorflow as tf\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM \n",
        "\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "sns.set_style(\"darkgrid\")\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "OdGN2XsgvhLI"
      },
      "outputs": [],
      "source": [
        "sys.path.append(os.path.abspath(os.path.join('../scripts')))\n",
        "from file_handler import FileHandler\n",
        "from log import get_logger  \n",
        "file_handler = FileHandler()\n",
        "my_logger = get_logger(\"LSTM_Modeling\")  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "DyRbkGoLvhLK"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-05-28 06:11:08,541 — FileHandler — DEBUG — file read as csv\n"
          ]
        }
      ],
      "source": [
        "train_data = file_handler.read_csv(\"../data/cleaned_train.csv\") "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aQ21ZPyB0Nv3"
      },
      "source": [
        "### Isolate the Rossmann Store Sales Dataset into Time Series Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 394
        },
        "id": "r07nZVX3vhLK",
        "outputId": "2d93bbb4-5873-4f8c-dfc4-95fdcc2611bb"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Sales</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Date</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2013-01-01</th>\n",
              "      <td>84.395871</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2013-01-02</th>\n",
              "      <td>6178.517489</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2013-01-03</th>\n",
              "      <td>5660.173094</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2013-01-04</th>\n",
              "      <td>5923.138117</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2013-01-05</th>\n",
              "      <td>5299.049327</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2013-01-06</th>\n",
              "      <td>119.357848</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2013-01-07</th>\n",
              "      <td>9377.999103</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2013-01-08</th>\n",
              "      <td>7742.737220</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2013-01-09</th>\n",
              "      <td>6887.256502</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2013-01-10</th>\n",
              "      <td>6894.729148</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                  Sales\n",
              "Date                   \n",
              "2013-01-01    84.395871\n",
              "2013-01-02  6178.517489\n",
              "2013-01-03  5660.173094\n",
              "2013-01-04  5923.138117\n",
              "2013-01-05  5299.049327\n",
              "2013-01-06   119.357848\n",
              "2013-01-07  9377.999103\n",
              "2013-01-08  7742.737220\n",
              "2013-01-09  6887.256502\n",
              "2013-01-10  6894.729148"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data = train_data.groupby(\"Date\").agg({\"Sales\": \"mean\"})\n",
        "data.head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LxBp-3QS0ZQB"
      },
      "source": [
        "### Check if the Time Series Data is Stationary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vujFduYx0aX_",
        "outputId": "75defc62-3c61-406c-d965-5df76bf542c2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ADF Statistic: -5.751282394418175\n",
            "p-value: 5.958430360162239e-07\n"
          ]
        }
      ],
      "source": [
        "adfResult = adfuller(data.Sales.values, autolag='AIC')\n",
        "print(f'ADF Statistic: {adfResult[0]}')\n",
        "print(f'p-value: {adfResult[1]}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pUHW4Ene0e4f"
      },
      "source": [
        "Since the p-value is less than 0.05, the data is not Stationary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 394
        },
        "id": "5Apwp5bP0aVv",
        "outputId": "2125ead1-7885-4a8e-a554-028c6753c245"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-05-28 06:11:09,300 — LSTM_Modeling — DEBUG — Data scaled successfully.\n",
            "2022-05-28 06:11:09,300 — LSTM_Modeling — DEBUG — Data scaled successfully.\n",
            "2022-05-28 06:11:09,300 — LSTM_Modeling — DEBUG — Data scaled successfully.\n",
            "2022-05-28 06:11:09,300 — LSTM_Modeling — DEBUG — Data scaled successfully.\n"
          ]
        }
      ],
      "source": [
        "scaler = MinMaxScaler()\n",
        "\n",
        "try:\n",
        "    scaler.fit(data.Sales.values.reshape([-1, 1]))\n",
        "    scaled_array = scaler.transform(data.Sales.values.reshape(-1, 1))\n",
        "    data['SalesScaled'] = scaled_array\n",
        "    data.tail(10)\n",
        "\n",
        "    my_logger.debug(\"Data scaled successfully.\")\n",
        "\n",
        "except Exception as e:\n",
        "    my_logger.exception(f\"Scaling error, {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o6rJ7teT0idA",
        "outputId": "ccdeeca9-d05a-486c-ca01-2a41f1f0da1c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ADF Statistic: -5.7512823944181894\n",
            "p-value: 5.958430360161802e-07\n"
          ]
        }
      ],
      "source": [
        "# Check if Scaled Sales is Stationary\n",
        "\n",
        "adfResult = adfuller(data.SalesScaled.values, autolag='AIC')\n",
        "print(f'ADF Statistic: {adfResult[0]}')\n",
        "print(f'p-value: {adfResult[1]}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Differencing the Time Series Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0      0.506975\n",
              "1     -0.043121\n",
              "2      0.021876\n",
              "3     -0.051918\n",
              "4     -0.430903\n",
              "         ...   \n",
              "936    0.760631\n",
              "937   -0.108573\n",
              "938   -0.043412\n",
              "939    0.022197\n",
              "940    0.089605\n",
              "Length: 941, dtype: float64"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Create a differenced series\n",
        "def difference(dataset, interval=1):\n",
        "    diff = list()\n",
        "    for i in range(interval, len(dataset)):\n",
        "        value = dataset[i] - dataset[i - interval]\n",
        "        diff.append(value)\n",
        "    return pd.Series(diff)\n",
        "\n",
        "salesScaledDiff = difference(data.SalesScaled.values)\n",
        "salesScaledDiff"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BfPpQSM60uoy"
      },
      "source": [
        "### Making the Data ready for Modeling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "0NWrqGYevhLN"
      },
      "outputs": [],
      "source": [
        "SIZE = len(data.SalesScaled)\n",
        "WINDOW_SIZE = 48\n",
        "BATCH_SIZE = SIZE - WINDOW_SIZE * 2\n",
        "EPOCHS = 200"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-sxjIDupvhLO",
        "outputId": "814d96be-4c87-46e6-f14d-9cf3ae7c5290"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape of the training set date series:  (846, 1)\n",
            "Shape of the validation set date series:  (96, 1)\n",
            "\n",
            "Shape of the training set logarithm of sales series:  (846,)\n",
            "Shape of the validation set logarithm of sales series in a stateless LSTM:  (96,)\n"
          ]
        }
      ],
      "source": [
        "DateTrain = data.index.values[0:BATCH_SIZE]\n",
        "DateValid = data.index.values[BATCH_SIZE:]\n",
        "XTrain = data.SalesScaled.values[0:BATCH_SIZE].astype('float32')\n",
        "XValid = data.SalesScaled.values[BATCH_SIZE:].astype('float32')\n",
        "\n",
        "# Obtain shapes for vectors of size (,1) for dates series\n",
        "\n",
        "DateTrain = np.reshape(DateTrain, (-1, 1))\n",
        "DateValid = np.reshape(DateValid, (-1, 1))\n",
        "\n",
        "print(\"Shape of the training set date series: \", DateTrain.shape)\n",
        "print(\"Shape of the validation set date series: \", DateValid.shape)\n",
        "print()\n",
        "print(\"Shape of the training set logarithm of sales series: \", XTrain.shape)\n",
        "print(\"Shape of the validation set logarithm of sales series in a stateless LSTM: \", XValid.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D-YwAj4-vhLR",
        "outputId": "7d61084c-4d24-4673-f04f-dc6e899f6e7c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "TensorShape([846, 1])"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tf.random.set_seed(1234)\n",
        "# add extra dimension\n",
        "series = tf.expand_dims(XTrain, axis=-1)\n",
        "series.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ppB31C0MvhLT",
        "outputId": "d552a4c5-b20f-468c-de0d-99f9d34129da"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<TensorSliceDataset element_spec=TensorSpec(shape=(1,), dtype=tf.float32, name=None)>"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# create tensor from each individual element\n",
        "dataset = tf.data.Dataset.from_tensor_slices(series)\n",
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "molbGZIJvhLV"
      },
      "outputs": [],
      "source": [
        "# takes a window_size + 1 chunk from the slices\n",
        "dataset = dataset.window(WINDOW_SIZE + 1, shift=1, drop_remainder=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t7JtdgE1vhLW",
        "outputId": "656dd218-f079-4010-daf0-2fa3bf521e1c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0, 1, 2, 3, 4]\n",
            "[1, 2, 3, 4, 5]\n",
            "[2, 3, 4, 5, 6]\n",
            "[3, 4, 5, 6, 7]\n",
            "[4, 5, 6, 7, 8]\n",
            "[5, 6, 7, 8, 9]\n"
          ]
        }
      ],
      "source": [
        "datasetEx = tf.data.Dataset.from_tensor_slices(tf.range(10))\n",
        "datasetEx = datasetEx.window(5, shift=1, drop_remainder=True)\n",
        "for window in datasetEx:\n",
        "    print([elem.numpy() for elem in window])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "JTh-MfzYvhLX"
      },
      "outputs": [],
      "source": [
        "dataset = dataset.flat_map(lambda window: window.batch(WINDOW_SIZE + 1))\n",
        "dataset = dataset.map(lambda window: (window[:-1], window[-1:]))\n",
        "dataset = dataset.batch(BATCH_SIZE).prefetch(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "lFXrq5YkvhLZ"
      },
      "outputs": [],
      "source": [
        "def windowed_dataset(series, window_size=WINDOW_SIZE, batch_size=BATCH_SIZE):\n",
        "  series = tf.expand_dims(series, axis=-1)\n",
        "  dataset = tf.data.Dataset.from_tensor_slices(series)\n",
        "  dataset = dataset.window(window_size + 1, shift=1, drop_remainder=True)\n",
        "  dataset = dataset.flat_map(lambda window: window.batch(window_size + 1))\n",
        "  dataset = dataset.map(lambda window: (window[:-1], window[-1:]))\n",
        "  dataset = dataset.batch(batch_size).prefetch(1)\n",
        "  return dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "Tc5GWnP-vhLa"
      },
      "outputs": [],
      "source": [
        "DatasetTrain = windowed_dataset(XTrain)\n",
        "DatasetVal = windowed_dataset(XValid)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Model Building"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kKf2XXeFvhLa",
        "outputId": "1bfc6d99-7de4-4be5-cb14-d81b398f80c9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " lstm (LSTM)                 (None, None, 8)           320       \n",
            "                                                                 \n",
            " lstm_1 (LSTM)               (None, 4)                 208       \n",
            "                                                                 \n",
            " dense (Dense)               (None, 1)                 5         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 533\n",
            "Trainable params: 533\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model = Sequential()\n",
        "model.add(LSTM(8, input_shape=[None, 1], return_sequences=True))\n",
        "model.add(LSTM(4, input_shape=[None, 1]))\n",
        "model.add(Dense(1))\n",
        "model.compile(loss=\"huber_loss\", optimizer='adam')\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Training the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oNUblnTsvhLc",
        "outputId": "6d1af11b-2dff-4522-d2c4-1fbdbe58fd78"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.2148 - val_loss: 0.2269\n",
            "Epoch 2/200\n",
            "1/1 [==============================] - 0s 158ms/step - loss: 0.2083 - val_loss: 0.2202\n",
            "Epoch 3/200\n",
            "1/1 [==============================] - 0s 155ms/step - loss: 0.2020 - val_loss: 0.2137\n",
            "Epoch 4/200\n",
            "1/1 [==============================] - 0s 153ms/step - loss: 0.1959 - val_loss: 0.2074\n",
            "Epoch 5/200\n",
            "1/1 [==============================] - 0s 156ms/step - loss: 0.1900 - val_loss: 0.2012\n",
            "Epoch 6/200\n",
            "1/1 [==============================] - 0s 168ms/step - loss: 0.1843 - val_loss: 0.1953\n",
            "Epoch 7/200\n",
            "1/1 [==============================] - 0s 145ms/step - loss: 0.1788 - val_loss: 0.1896\n",
            "Epoch 8/200\n",
            "1/1 [==============================] - 0s 160ms/step - loss: 0.1735 - val_loss: 0.1840\n",
            "Epoch 9/200\n",
            "1/1 [==============================] - 0s 160ms/step - loss: 0.1683 - val_loss: 0.1786\n",
            "Epoch 10/200\n",
            "1/1 [==============================] - 0s 161ms/step - loss: 0.1633 - val_loss: 0.1734\n",
            "Epoch 11/200\n",
            "1/1 [==============================] - 0s 147ms/step - loss: 0.1584 - val_loss: 0.1683\n",
            "Epoch 12/200\n",
            "1/1 [==============================] - 0s 154ms/step - loss: 0.1537 - val_loss: 0.1633\n",
            "Epoch 13/200\n",
            "1/1 [==============================] - 0s 156ms/step - loss: 0.1491 - val_loss: 0.1585\n",
            "Epoch 14/200\n",
            "1/1 [==============================] - 0s 195ms/step - loss: 0.1446 - val_loss: 0.1539\n",
            "Epoch 15/200\n",
            "1/1 [==============================] - 0s 188ms/step - loss: 0.1403 - val_loss: 0.1493\n",
            "Epoch 16/200\n",
            "1/1 [==============================] - 0s 167ms/step - loss: 0.1361 - val_loss: 0.1448\n",
            "Epoch 17/200\n",
            "1/1 [==============================] - 0s 164ms/step - loss: 0.1319 - val_loss: 0.1405\n",
            "Epoch 18/200\n",
            "1/1 [==============================] - 0s 206ms/step - loss: 0.1279 - val_loss: 0.1362\n",
            "Epoch 19/200\n",
            "1/1 [==============================] - 0s 200ms/step - loss: 0.1240 - val_loss: 0.1321\n",
            "Epoch 20/200\n",
            "1/1 [==============================] - 0s 159ms/step - loss: 0.1201 - val_loss: 0.1280\n",
            "Epoch 21/200\n",
            "1/1 [==============================] - 0s 150ms/step - loss: 0.1163 - val_loss: 0.1240\n",
            "Epoch 22/200\n",
            "1/1 [==============================] - 0s 149ms/step - loss: 0.1126 - val_loss: 0.1200\n",
            "Epoch 23/200\n",
            "1/1 [==============================] - 0s 145ms/step - loss: 0.1090 - val_loss: 0.1161\n",
            "Epoch 24/200\n",
            "1/1 [==============================] - 0s 190ms/step - loss: 0.1054 - val_loss: 0.1123\n",
            "Epoch 25/200\n",
            "1/1 [==============================] - 0s 157ms/step - loss: 0.1019 - val_loss: 0.1086\n",
            "Epoch 26/200\n",
            "1/1 [==============================] - 0s 150ms/step - loss: 0.0985 - val_loss: 0.1049\n",
            "Epoch 27/200\n",
            "1/1 [==============================] - 0s 147ms/step - loss: 0.0951 - val_loss: 0.1012\n",
            "Epoch 28/200\n",
            "1/1 [==============================] - 0s 146ms/step - loss: 0.0918 - val_loss: 0.0977\n",
            "Epoch 29/200\n",
            "1/1 [==============================] - 0s 158ms/step - loss: 0.0885 - val_loss: 0.0941\n",
            "Epoch 30/200\n",
            "1/1 [==============================] - 0s 189ms/step - loss: 0.0853 - val_loss: 0.0907\n",
            "Epoch 31/200\n",
            "1/1 [==============================] - 0s 210ms/step - loss: 0.0821 - val_loss: 0.0873\n",
            "Epoch 32/200\n",
            "1/1 [==============================] - 0s 253ms/step - loss: 0.0790 - val_loss: 0.0839\n",
            "Epoch 33/200\n",
            "1/1 [==============================] - 0s 162ms/step - loss: 0.0760 - val_loss: 0.0807\n",
            "Epoch 34/200\n",
            "1/1 [==============================] - 0s 184ms/step - loss: 0.0730 - val_loss: 0.0774\n",
            "Epoch 35/200\n",
            "1/1 [==============================] - 0s 193ms/step - loss: 0.0701 - val_loss: 0.0743\n",
            "Epoch 36/200\n",
            "1/1 [==============================] - 0s 161ms/step - loss: 0.0673 - val_loss: 0.0712\n",
            "Epoch 37/200\n",
            "1/1 [==============================] - 0s 155ms/step - loss: 0.0645 - val_loss: 0.0682\n",
            "Epoch 38/200\n",
            "1/1 [==============================] - 0s 165ms/step - loss: 0.0619 - val_loss: 0.0653\n",
            "Epoch 39/200\n",
            "1/1 [==============================] - 0s 160ms/step - loss: 0.0592 - val_loss: 0.0624\n",
            "Epoch 40/200\n",
            "1/1 [==============================] - 0s 157ms/step - loss: 0.0567 - val_loss: 0.0597\n",
            "Epoch 41/200\n",
            "1/1 [==============================] - 0s 165ms/step - loss: 0.0543 - val_loss: 0.0570\n",
            "Epoch 42/200\n",
            "1/1 [==============================] - 0s 173ms/step - loss: 0.0519 - val_loss: 0.0544\n",
            "Epoch 43/200\n",
            "1/1 [==============================] - 0s 168ms/step - loss: 0.0496 - val_loss: 0.0519\n",
            "Epoch 44/200\n",
            "1/1 [==============================] - 0s 169ms/step - loss: 0.0474 - val_loss: 0.0495\n",
            "Epoch 45/200\n",
            "1/1 [==============================] - 0s 154ms/step - loss: 0.0454 - val_loss: 0.0472\n",
            "Epoch 46/200\n",
            "1/1 [==============================] - 0s 169ms/step - loss: 0.0434 - val_loss: 0.0450\n",
            "Epoch 47/200\n",
            "1/1 [==============================] - 0s 162ms/step - loss: 0.0415 - val_loss: 0.0430\n",
            "Epoch 48/200\n",
            "1/1 [==============================] - 0s 167ms/step - loss: 0.0398 - val_loss: 0.0410\n",
            "Epoch 49/200\n",
            "1/1 [==============================] - 0s 182ms/step - loss: 0.0381 - val_loss: 0.0392\n",
            "Epoch 50/200\n",
            "1/1 [==============================] - 0s 176ms/step - loss: 0.0366 - val_loss: 0.0374\n",
            "Epoch 51/200\n",
            "1/1 [==============================] - 0s 159ms/step - loss: 0.0352 - val_loss: 0.0358\n",
            "Epoch 52/200\n",
            "1/1 [==============================] - 0s 165ms/step - loss: 0.0339 - val_loss: 0.0344\n",
            "Epoch 53/200\n",
            "1/1 [==============================] - 0s 156ms/step - loss: 0.0327 - val_loss: 0.0330\n",
            "Epoch 54/200\n",
            "1/1 [==============================] - 0s 159ms/step - loss: 0.0316 - val_loss: 0.0318\n",
            "Epoch 55/200\n",
            "1/1 [==============================] - 0s 156ms/step - loss: 0.0307 - val_loss: 0.0307\n",
            "Epoch 56/200\n",
            "1/1 [==============================] - 0s 162ms/step - loss: 0.0299 - val_loss: 0.0297\n",
            "Epoch 57/200\n",
            "1/1 [==============================] - 0s 161ms/step - loss: 0.0292 - val_loss: 0.0289\n",
            "Epoch 58/200\n",
            "1/1 [==============================] - 0s 160ms/step - loss: 0.0286 - val_loss: 0.0281\n",
            "Epoch 59/200\n",
            "1/1 [==============================] - 0s 152ms/step - loss: 0.0282 - val_loss: 0.0275\n",
            "Epoch 60/200\n",
            "1/1 [==============================] - 0s 167ms/step - loss: 0.0278 - val_loss: 0.0270\n",
            "Epoch 61/200\n",
            "1/1 [==============================] - 0s 166ms/step - loss: 0.0275 - val_loss: 0.0266\n",
            "Epoch 62/200\n",
            "1/1 [==============================] - 0s 156ms/step - loss: 0.0273 - val_loss: 0.0263\n",
            "Epoch 63/200\n",
            "1/1 [==============================] - 0s 164ms/step - loss: 0.0272 - val_loss: 0.0260\n",
            "Epoch 64/200\n",
            "1/1 [==============================] - 0s 162ms/step - loss: 0.0272 - val_loss: 0.0258\n",
            "Epoch 65/200\n",
            "1/1 [==============================] - 0s 155ms/step - loss: 0.0272 - val_loss: 0.0257\n",
            "Epoch 66/200\n",
            "1/1 [==============================] - 0s 163ms/step - loss: 0.0272 - val_loss: 0.0256\n",
            "Epoch 67/200\n",
            "1/1 [==============================] - 0s 169ms/step - loss: 0.0273 - val_loss: 0.0256\n",
            "Epoch 68/200\n",
            "1/1 [==============================] - 0s 166ms/step - loss: 0.0273 - val_loss: 0.0255\n",
            "Epoch 69/200\n",
            "1/1 [==============================] - 0s 216ms/step - loss: 0.0274 - val_loss: 0.0255\n",
            "Epoch 70/200\n",
            "1/1 [==============================] - 0s 216ms/step - loss: 0.0275 - val_loss: 0.0255\n",
            "Epoch 71/200\n",
            "1/1 [==============================] - 0s 167ms/step - loss: 0.0276 - val_loss: 0.0255\n",
            "Epoch 72/200\n",
            "1/1 [==============================] - 0s 159ms/step - loss: 0.0277 - val_loss: 0.0255\n",
            "Epoch 73/200\n",
            "1/1 [==============================] - 0s 164ms/step - loss: 0.0277 - val_loss: 0.0255\n",
            "Epoch 74/200\n",
            "1/1 [==============================] - 0s 175ms/step - loss: 0.0277 - val_loss: 0.0255\n",
            "Epoch 75/200\n",
            "1/1 [==============================] - 0s 171ms/step - loss: 0.0277 - val_loss: 0.0255\n",
            "Epoch 76/200\n",
            "1/1 [==============================] - 0s 169ms/step - loss: 0.0277 - val_loss: 0.0255\n",
            "Epoch 77/200\n",
            "1/1 [==============================] - 0s 168ms/step - loss: 0.0277 - val_loss: 0.0255\n",
            "Epoch 78/200\n",
            "1/1 [==============================] - 0s 174ms/step - loss: 0.0277 - val_loss: 0.0255\n",
            "Epoch 79/200\n",
            "1/1 [==============================] - 0s 172ms/step - loss: 0.0276 - val_loss: 0.0255\n",
            "Epoch 80/200\n",
            "1/1 [==============================] - 0s 173ms/step - loss: 0.0276 - val_loss: 0.0255\n",
            "Epoch 81/200\n",
            "1/1 [==============================] - 0s 171ms/step - loss: 0.0275 - val_loss: 0.0255\n",
            "Epoch 82/200\n",
            "1/1 [==============================] - 0s 179ms/step - loss: 0.0275 - val_loss: 0.0255\n",
            "Epoch 83/200\n",
            "1/1 [==============================] - 0s 176ms/step - loss: 0.0274 - val_loss: 0.0255\n",
            "Epoch 84/200\n",
            "1/1 [==============================] - 0s 178ms/step - loss: 0.0274 - val_loss: 0.0256\n",
            "Epoch 85/200\n",
            "1/1 [==============================] - 0s 171ms/step - loss: 0.0273 - val_loss: 0.0256\n",
            "Epoch 86/200\n",
            "1/1 [==============================] - 0s 179ms/step - loss: 0.0273 - val_loss: 0.0256\n",
            "Epoch 87/200\n",
            "1/1 [==============================] - 0s 170ms/step - loss: 0.0273 - val_loss: 0.0256\n",
            "Epoch 88/200\n",
            "1/1 [==============================] - 0s 180ms/step - loss: 0.0272 - val_loss: 0.0257\n",
            "Epoch 89/200\n",
            "1/1 [==============================] - 0s 171ms/step - loss: 0.0272 - val_loss: 0.0257\n",
            "Epoch 90/200\n",
            "1/1 [==============================] - 0s 179ms/step - loss: 0.0272 - val_loss: 0.0258\n",
            "Epoch 91/200\n",
            "1/1 [==============================] - 0s 167ms/step - loss: 0.0272 - val_loss: 0.0258\n",
            "Epoch 92/200\n",
            "1/1 [==============================] - 0s 173ms/step - loss: 0.0272 - val_loss: 0.0258\n",
            "Epoch 93/200\n",
            "1/1 [==============================] - 0s 178ms/step - loss: 0.0272 - val_loss: 0.0259\n",
            "Epoch 94/200\n",
            "1/1 [==============================] - 0s 170ms/step - loss: 0.0272 - val_loss: 0.0259\n",
            "Epoch 95/200\n",
            "1/1 [==============================] - 0s 167ms/step - loss: 0.0272 - val_loss: 0.0260\n",
            "Epoch 96/200\n",
            "1/1 [==============================] - 0s 175ms/step - loss: 0.0272 - val_loss: 0.0260\n",
            "Epoch 97/200\n",
            "1/1 [==============================] - 0s 169ms/step - loss: 0.0272 - val_loss: 0.0260\n",
            "Epoch 98/200\n",
            "1/1 [==============================] - 0s 182ms/step - loss: 0.0272 - val_loss: 0.0260\n",
            "Epoch 99/200\n",
            "1/1 [==============================] - 0s 168ms/step - loss: 0.0272 - val_loss: 0.0261\n",
            "Epoch 100/200\n",
            "1/1 [==============================] - 0s 183ms/step - loss: 0.0272 - val_loss: 0.0261\n",
            "Epoch 101/200\n",
            "1/1 [==============================] - 0s 172ms/step - loss: 0.0272 - val_loss: 0.0261\n",
            "Epoch 102/200\n",
            "1/1 [==============================] - 0s 173ms/step - loss: 0.0272 - val_loss: 0.0261\n",
            "Epoch 103/200\n",
            "1/1 [==============================] - 0s 171ms/step - loss: 0.0272 - val_loss: 0.0261\n",
            "Epoch 104/200\n",
            "1/1 [==============================] - 0s 177ms/step - loss: 0.0272 - val_loss: 0.0261\n",
            "Epoch 105/200\n",
            "1/1 [==============================] - 0s 172ms/step - loss: 0.0272 - val_loss: 0.0261\n",
            "Epoch 106/200\n",
            "1/1 [==============================] - 0s 175ms/step - loss: 0.0272 - val_loss: 0.0261\n",
            "Epoch 107/200\n",
            "1/1 [==============================] - 0s 172ms/step - loss: 0.0272 - val_loss: 0.0261\n",
            "Epoch 108/200\n",
            "1/1 [==============================] - 0s 178ms/step - loss: 0.0272 - val_loss: 0.0261\n",
            "Epoch 109/200\n",
            "1/1 [==============================] - 0s 171ms/step - loss: 0.0272 - val_loss: 0.0261\n",
            "Epoch 110/200\n",
            "1/1 [==============================] - 0s 181ms/step - loss: 0.0272 - val_loss: 0.0261\n",
            "Epoch 111/200\n",
            "1/1 [==============================] - 0s 169ms/step - loss: 0.0272 - val_loss: 0.0260\n",
            "Epoch 112/200\n",
            "1/1 [==============================] - 0s 177ms/step - loss: 0.0272 - val_loss: 0.0260\n",
            "Epoch 113/200\n",
            "1/1 [==============================] - 0s 178ms/step - loss: 0.0272 - val_loss: 0.0260\n",
            "Epoch 114/200\n",
            "1/1 [==============================] - 0s 168ms/step - loss: 0.0272 - val_loss: 0.0260\n",
            "Epoch 115/200\n",
            "1/1 [==============================] - 0s 179ms/step - loss: 0.0272 - val_loss: 0.0260\n",
            "Epoch 116/200\n",
            "1/1 [==============================] - 0s 169ms/step - loss: 0.0272 - val_loss: 0.0260\n",
            "Epoch 117/200\n",
            "1/1 [==============================] - 0s 186ms/step - loss: 0.0272 - val_loss: 0.0259\n",
            "Epoch 118/200\n",
            "1/1 [==============================] - 0s 175ms/step - loss: 0.0272 - val_loss: 0.0259\n",
            "Epoch 119/200\n",
            "1/1 [==============================] - 0s 172ms/step - loss: 0.0272 - val_loss: 0.0259\n",
            "Epoch 120/200\n",
            "1/1 [==============================] - 0s 169ms/step - loss: 0.0271 - val_loss: 0.0259\n",
            "Epoch 121/200\n",
            "1/1 [==============================] - 0s 176ms/step - loss: 0.0271 - val_loss: 0.0259\n",
            "Epoch 122/200\n",
            "1/1 [==============================] - 0s 176ms/step - loss: 0.0271 - val_loss: 0.0259\n",
            "Epoch 123/200\n",
            "1/1 [==============================] - 0s 173ms/step - loss: 0.0271 - val_loss: 0.0259\n",
            "Epoch 124/200\n",
            "1/1 [==============================] - 0s 174ms/step - loss: 0.0271 - val_loss: 0.0259\n",
            "Epoch 125/200\n",
            "1/1 [==============================] - 0s 178ms/step - loss: 0.0271 - val_loss: 0.0258\n",
            "Epoch 126/200\n",
            "1/1 [==============================] - 0s 168ms/step - loss: 0.0271 - val_loss: 0.0258\n",
            "Epoch 127/200\n",
            "1/1 [==============================] - 0s 177ms/step - loss: 0.0271 - val_loss: 0.0258\n",
            "Epoch 128/200\n",
            "1/1 [==============================] - 0s 173ms/step - loss: 0.0271 - val_loss: 0.0258\n",
            "Epoch 129/200\n",
            "1/1 [==============================] - 0s 176ms/step - loss: 0.0271 - val_loss: 0.0258\n",
            "Epoch 130/200\n",
            "1/1 [==============================] - 0s 169ms/step - loss: 0.0271 - val_loss: 0.0258\n",
            "Epoch 131/200\n",
            "1/1 [==============================] - 0s 175ms/step - loss: 0.0271 - val_loss: 0.0258\n",
            "Epoch 132/200\n",
            "1/1 [==============================] - 0s 183ms/step - loss: 0.0271 - val_loss: 0.0258\n",
            "Epoch 133/200\n",
            "1/1 [==============================] - 0s 181ms/step - loss: 0.0271 - val_loss: 0.0258\n",
            "Epoch 134/200\n",
            "1/1 [==============================] - 0s 174ms/step - loss: 0.0271 - val_loss: 0.0258\n",
            "Epoch 135/200\n",
            "1/1 [==============================] - 0s 182ms/step - loss: 0.0271 - val_loss: 0.0258\n",
            "Epoch 136/200\n",
            "1/1 [==============================] - 0s 170ms/step - loss: 0.0271 - val_loss: 0.0258\n",
            "Epoch 137/200\n",
            "1/1 [==============================] - 0s 189ms/step - loss: 0.0271 - val_loss: 0.0258\n",
            "Epoch 138/200\n",
            "1/1 [==============================] - 0s 179ms/step - loss: 0.0271 - val_loss: 0.0258\n",
            "Epoch 139/200\n",
            "1/1 [==============================] - 0s 191ms/step - loss: 0.0271 - val_loss: 0.0258\n",
            "Epoch 140/200\n",
            "1/1 [==============================] - 0s 173ms/step - loss: 0.0271 - val_loss: 0.0258\n",
            "Epoch 141/200\n",
            "1/1 [==============================] - 0s 181ms/step - loss: 0.0271 - val_loss: 0.0258\n",
            "Epoch 142/200\n",
            "1/1 [==============================] - 0s 175ms/step - loss: 0.0271 - val_loss: 0.0258\n",
            "Epoch 143/200\n",
            "1/1 [==============================] - 0s 176ms/step - loss: 0.0271 - val_loss: 0.0258\n",
            "Epoch 144/200\n",
            "1/1 [==============================] - 0s 190ms/step - loss: 0.0271 - val_loss: 0.0258\n",
            "Epoch 145/200\n",
            "1/1 [==============================] - 0s 175ms/step - loss: 0.0271 - val_loss: 0.0258\n",
            "Epoch 146/200\n",
            "1/1 [==============================] - 0s 186ms/step - loss: 0.0271 - val_loss: 0.0258\n",
            "Epoch 147/200\n",
            "1/1 [==============================] - 0s 180ms/step - loss: 0.0271 - val_loss: 0.0258\n",
            "Epoch 148/200\n",
            "1/1 [==============================] - 0s 285ms/step - loss: 0.0271 - val_loss: 0.0258\n",
            "Epoch 149/200\n",
            "1/1 [==============================] - 0s 175ms/step - loss: 0.0271 - val_loss: 0.0258\n",
            "Epoch 150/200\n",
            "1/1 [==============================] - 0s 175ms/step - loss: 0.0271 - val_loss: 0.0258\n",
            "Epoch 151/200\n",
            "1/1 [==============================] - 0s 178ms/step - loss: 0.0271 - val_loss: 0.0258\n",
            "Epoch 152/200\n",
            "1/1 [==============================] - 0s 185ms/step - loss: 0.0271 - val_loss: 0.0258\n",
            "Epoch 153/200\n",
            "1/1 [==============================] - 0s 180ms/step - loss: 0.0271 - val_loss: 0.0258\n",
            "Epoch 154/200\n",
            "1/1 [==============================] - 0s 179ms/step - loss: 0.0271 - val_loss: 0.0258\n",
            "Epoch 155/200\n",
            "1/1 [==============================] - 0s 189ms/step - loss: 0.0271 - val_loss: 0.0258\n",
            "Epoch 156/200\n",
            "1/1 [==============================] - 0s 176ms/step - loss: 0.0271 - val_loss: 0.0258\n",
            "Epoch 157/200\n",
            "1/1 [==============================] - 0s 191ms/step - loss: 0.0271 - val_loss: 0.0258\n",
            "Epoch 158/200\n",
            "1/1 [==============================] - 0s 180ms/step - loss: 0.0271 - val_loss: 0.0258\n",
            "Epoch 159/200\n",
            "1/1 [==============================] - 0s 180ms/step - loss: 0.0271 - val_loss: 0.0258\n",
            "Epoch 160/200\n",
            "1/1 [==============================] - 0s 183ms/step - loss: 0.0271 - val_loss: 0.0258\n",
            "Epoch 161/200\n",
            "1/1 [==============================] - 0s 176ms/step - loss: 0.0271 - val_loss: 0.0258\n",
            "Epoch 162/200\n",
            "1/1 [==============================] - 0s 184ms/step - loss: 0.0271 - val_loss: 0.0258\n",
            "Epoch 163/200\n",
            "1/1 [==============================] - 0s 177ms/step - loss: 0.0271 - val_loss: 0.0258\n",
            "Epoch 164/200\n",
            "1/1 [==============================] - 0s 178ms/step - loss: 0.0271 - val_loss: 0.0258\n",
            "Epoch 165/200\n",
            "1/1 [==============================] - 0s 184ms/step - loss: 0.0271 - val_loss: 0.0258\n",
            "Epoch 166/200\n",
            "1/1 [==============================] - 0s 175ms/step - loss: 0.0271 - val_loss: 0.0258\n",
            "Epoch 167/200\n",
            "1/1 [==============================] - 0s 178ms/step - loss: 0.0271 - val_loss: 0.0258\n",
            "Epoch 168/200\n",
            "1/1 [==============================] - 0s 172ms/step - loss: 0.0271 - val_loss: 0.0258\n",
            "Epoch 169/200\n",
            "1/1 [==============================] - 0s 180ms/step - loss: 0.0271 - val_loss: 0.0258\n",
            "Epoch 170/200\n",
            "1/1 [==============================] - 0s 170ms/step - loss: 0.0271 - val_loss: 0.0258\n",
            "Epoch 171/200\n",
            "1/1 [==============================] - 0s 195ms/step - loss: 0.0271 - val_loss: 0.0258\n",
            "Epoch 172/200\n",
            "1/1 [==============================] - 0s 182ms/step - loss: 0.0271 - val_loss: 0.0258\n",
            "Epoch 173/200\n",
            "1/1 [==============================] - 0s 190ms/step - loss: 0.0271 - val_loss: 0.0258\n",
            "Epoch 174/200\n",
            "1/1 [==============================] - 0s 174ms/step - loss: 0.0271 - val_loss: 0.0258\n",
            "Epoch 175/200\n",
            "1/1 [==============================] - 0s 186ms/step - loss: 0.0271 - val_loss: 0.0258\n",
            "Epoch 176/200\n",
            "1/1 [==============================] - 0s 177ms/step - loss: 0.0271 - val_loss: 0.0258\n",
            "Epoch 177/200\n",
            "1/1 [==============================] - 0s 175ms/step - loss: 0.0271 - val_loss: 0.0258\n",
            "Epoch 178/200\n",
            "1/1 [==============================] - 0s 179ms/step - loss: 0.0271 - val_loss: 0.0258\n",
            "Epoch 179/200\n",
            "1/1 [==============================] - 0s 181ms/step - loss: 0.0271 - val_loss: 0.0258\n",
            "Epoch 180/200\n",
            "1/1 [==============================] - 0s 179ms/step - loss: 0.0271 - val_loss: 0.0258\n",
            "Epoch 181/200\n",
            "1/1 [==============================] - 0s 181ms/step - loss: 0.0271 - val_loss: 0.0258\n",
            "Epoch 182/200\n",
            "1/1 [==============================] - 0s 175ms/step - loss: 0.0271 - val_loss: 0.0258\n",
            "Epoch 183/200\n",
            "1/1 [==============================] - 0s 213ms/step - loss: 0.0271 - val_loss: 0.0258\n",
            "Epoch 184/200\n",
            "1/1 [==============================] - 0s 177ms/step - loss: 0.0271 - val_loss: 0.0258\n",
            "Epoch 185/200\n",
            "1/1 [==============================] - 0s 181ms/step - loss: 0.0271 - val_loss: 0.0258\n",
            "Epoch 186/200\n",
            "1/1 [==============================] - 0s 174ms/step - loss: 0.0271 - val_loss: 0.0258\n",
            "Epoch 187/200\n",
            "1/1 [==============================] - 0s 177ms/step - loss: 0.0271 - val_loss: 0.0258\n",
            "Epoch 188/200\n",
            "1/1 [==============================] - 0s 190ms/step - loss: 0.0271 - val_loss: 0.0258\n",
            "Epoch 189/200\n",
            "1/1 [==============================] - 0s 176ms/step - loss: 0.0271 - val_loss: 0.0258\n",
            "Epoch 190/200\n",
            "1/1 [==============================] - 0s 174ms/step - loss: 0.0271 - val_loss: 0.0258\n",
            "Epoch 191/200\n",
            "1/1 [==============================] - 0s 183ms/step - loss: 0.0271 - val_loss: 0.0258\n",
            "Epoch 192/200\n",
            "1/1 [==============================] - 0s 172ms/step - loss: 0.0271 - val_loss: 0.0258\n",
            "Epoch 193/200\n",
            "1/1 [==============================] - 0s 179ms/step - loss: 0.0271 - val_loss: 0.0258\n",
            "Epoch 194/200\n",
            "1/1 [==============================] - 0s 175ms/step - loss: 0.0271 - val_loss: 0.0258\n",
            "Epoch 195/200\n",
            "1/1 [==============================] - 0s 177ms/step - loss: 0.0271 - val_loss: 0.0258\n",
            "Epoch 196/200\n",
            "1/1 [==============================] - 0s 180ms/step - loss: 0.0271 - val_loss: 0.0258\n",
            "Epoch 197/200\n",
            "1/1 [==============================] - 0s 176ms/step - loss: 0.0271 - val_loss: 0.0258\n",
            "Epoch 198/200\n",
            "1/1 [==============================] - 0s 178ms/step - loss: 0.0271 - val_loss: 0.0258\n",
            "Epoch 199/200\n",
            "1/1 [==============================] - 0s 175ms/step - loss: 0.0271 - val_loss: 0.0258\n",
            "Epoch 200/200\n",
            "1/1 [==============================] - 0s 168ms/step - loss: 0.0271 - val_loss: 0.0258\n"
          ]
        }
      ],
      "source": [
        "history = model.fit(DatasetTrain, epochs=EPOCHS, validation_data=DatasetVal, verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "METS1_c4vhLc"
      },
      "outputs": [],
      "source": [
        "from time import gmtime, strftime\n",
        "\n",
        "time = strftime(\"%Y-%m-%d\", gmtime())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "CzWb7k41vhLd"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: ../models/LSTM_sales_prediction_model 2022-05-28.pkl\\assets\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: ../models/LSTM_sales_prediction_model 2022-05-28.pkl\\assets\n"
          ]
        }
      ],
      "source": [
        "model.save(f'../models/LSTM_sales_prediction_model {time}.pkl')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "LSTM_prediction.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "0f08ea943be090a14eff5269cda570ac55dcc0e2bd93317a14b7d7e20047a087"
    },
    "kernelspec": {
      "display_name": "Python 3.10.4 64-bit (windows store)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
